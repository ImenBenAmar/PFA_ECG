digraph {
	graph [size="33.15,33.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2052233802880 [label="
 (1, 5)" fillcolor=darkolivegreen1]
	2052233666880 [label=AddmmBackward0]
	2051951565248 -> 2052233666880
	2051822479920 [label="classifier.3.bias
 (5)" fillcolor=lightblue]
	2051822479920 -> 2051951565248
	2051951565248 [label=AccumulateGrad]
	2052233816880 -> 2052233666880
	2052233816880 [label=ReluBackward0]
	2052233817312 -> 2052233816880
	2052233817312 [label=AddmmBackward0]
	2052233817168 -> 2052233817312
	2051822473360 [label="classifier.0.bias
 (64)" fillcolor=lightblue]
	2051822473360 -> 2052233817168
	2052233817168 [label=AccumulateGrad]
	2052233817216 -> 2052233817312
	2052233817216 [label=CatBackward0]
	2052233817408 -> 2052233817216
	2052233817408 [label=ReluBackward0]
	2052233817600 -> 2052233817408
	2052233817600 [label=AddmmBackward0]
	2052233817696 -> 2052233817600
	2051822480560 [label="cnn_fc.1.bias
 (256)" fillcolor=lightblue]
	2051822480560 -> 2052233817696
	2052233817696 [label=AccumulateGrad]
	2052233817648 -> 2052233817600
	2052233817648 [label=ViewBackward0]
	2052233817840 -> 2052233817648
	2052233817840 [label=MaxPool2DWithIndicesBackward0]
	2052233817984 -> 2052233817840
	2052233817984 [label=ReluBackward0]
	2052233818080 -> 2052233817984
	2052233818080 [label=CudnnBatchNormBackward0]
	2052233818176 -> 2052233818080
	2052233818176 [label=ConvolutionBackward0]
	2052233818368 -> 2052233818176
	2052233818368 [label=ReluBackward0]
	2052233818560 -> 2052233818368
	2052233818560 [label=CudnnBatchNormBackward0]
	2052233818656 -> 2052233818560
	2052233818656 [label=ConvolutionBackward0]
	2052233818848 -> 2052233818656
	2052233818848 [label=MaxPool2DWithIndicesBackward0]
	2052233819040 -> 2052233818848
	2052233819040 [label=ReluBackward0]
	2052233819136 -> 2052233819040
	2052233819136 [label=CudnnBatchNormBackward0]
	2052233819232 -> 2052233819136
	2052233819232 [label=ConvolutionBackward0]
	2052233819424 -> 2052233819232
	2052233819424 [label=ReluBackward0]
	2052233819616 -> 2052233819424
	2052233819616 [label=CudnnBatchNormBackward0]
	2052233819712 -> 2052233819616
	2052233819712 [label=ConvolutionBackward0]
	2052233819904 -> 2052233819712
	2052233819904 [label=MaxPool2DWithIndicesBackward0]
	2052233820096 -> 2052233819904
	2052233820096 [label=ReluBackward0]
	2052233820192 -> 2052233820096
	2052233820192 [label=CudnnBatchNormBackward0]
	2052233820288 -> 2052233820192
	2052233820288 [label=ConvolutionBackward0]
	2052233820480 -> 2052233820288
	2052233820480 [label=ReluBackward0]
	2052233820672 -> 2052233820480
	2052233820672 [label=CudnnBatchNormBackward0]
	2052233820768 -> 2052233820672
	2052233820768 [label=ConvolutionBackward0]
	2052233820960 -> 2052233820768
	2051822424208 [label="conv1.0.weight
 (3, 1, 3, 3)" fillcolor=lightblue]
	2051822424208 -> 2052233820960
	2052233820960 [label=AccumulateGrad]
	2052233820912 -> 2052233820768
	2051822423728 [label="conv1.0.bias
 (3)" fillcolor=lightblue]
	2051822423728 -> 2052233820912
	2052233820912 [label=AccumulateGrad]
	2052233820720 -> 2052233820672
	2051949060784 [label="conv1.1.weight
 (3)" fillcolor=lightblue]
	2051949060784 -> 2052233820720
	2052233820720 [label=AccumulateGrad]
	2052233820576 -> 2052233820672
	2051822422448 [label="conv1.1.bias
 (3)" fillcolor=lightblue]
	2051822422448 -> 2052233820576
	2052233820576 [label=AccumulateGrad]
	2052233820432 -> 2052233820288
	2051822420768 [label="conv1.3.weight
 (16, 3, 1, 1)" fillcolor=lightblue]
	2051822420768 -> 2052233820432
	2052233820432 [label=AccumulateGrad]
	2052233820384 -> 2052233820288
	2051822420608 [label="conv1.3.bias
 (16)" fillcolor=lightblue]
	2051822420608 -> 2052233820384
	2052233820384 [label=AccumulateGrad]
	2052233820240 -> 2052233820192
	2051822420528 [label="conv1.4.weight
 (16)" fillcolor=lightblue]
	2051822420528 -> 2052233820240
	2052233820240 [label=AccumulateGrad]
	2052233820000 -> 2052233820192
	2051822420368 [label="conv1.4.bias
 (16)" fillcolor=lightblue]
	2051822420368 -> 2052233820000
	2052233820000 [label=AccumulateGrad]
	2052233819856 -> 2052233819712
	2051822430528 [label="conv2.0.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	2051822430528 -> 2052233819856
	2052233819856 [label=AccumulateGrad]
	2052233819808 -> 2052233819712
	2051822430448 [label="conv2.0.bias
 (16)" fillcolor=lightblue]
	2051822430448 -> 2052233819808
	2052233819808 [label=AccumulateGrad]
	2052233819664 -> 2052233819616
	2051822430368 [label="conv2.1.weight
 (16)" fillcolor=lightblue]
	2051822430368 -> 2052233819664
	2052233819664 [label=AccumulateGrad]
	2052233819520 -> 2052233819616
	2051822430288 [label="conv2.1.bias
 (16)" fillcolor=lightblue]
	2051822430288 -> 2052233819520
	2052233819520 [label=AccumulateGrad]
	2052233819376 -> 2052233819232
	2051822420128 [label="conv2.3.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	2051822420128 -> 2052233819376
	2052233819376 [label=AccumulateGrad]
	2052233819328 -> 2052233819232
	2051822420048 [label="conv2.3.bias
 (32)" fillcolor=lightblue]
	2051822420048 -> 2052233819328
	2052233819328 [label=AccumulateGrad]
	2052233819184 -> 2052233819136
	2051822435648 [label="conv2.4.weight
 (32)" fillcolor=lightblue]
	2051822435648 -> 2052233819184
	2052233819184 [label=AccumulateGrad]
	2052233818944 -> 2052233819136
	2051822435568 [label="conv2.4.bias
 (32)" fillcolor=lightblue]
	2051822435568 -> 2052233818944
	2052233818944 [label=AccumulateGrad]
	2052233818800 -> 2052233818656
	2051822481360 [label="conv3.0.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	2051822481360 -> 2052233818800
	2052233818800 [label=AccumulateGrad]
	2052233818752 -> 2052233818656
	2051822481280 [label="conv3.0.bias
 (32)" fillcolor=lightblue]
	2051822481280 -> 2052233818752
	2052233818752 [label=AccumulateGrad]
	2052233818608 -> 2052233818560
	2051822474640 [label="conv3.1.weight
 (32)" fillcolor=lightblue]
	2051822474640 -> 2052233818608
	2052233818608 [label=AccumulateGrad]
	2052233818464 -> 2052233818560
	2051822474560 [label="conv3.1.bias
 (32)" fillcolor=lightblue]
	2051822474560 -> 2052233818464
	2052233818464 [label=AccumulateGrad]
	2052233818320 -> 2052233818176
	2051822480960 [label="conv3.3.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	2051822480960 -> 2052233818320
	2052233818320 [label=AccumulateGrad]
	2052233818272 -> 2052233818176
	2051822474320 [label="conv3.3.bias
 (64)" fillcolor=lightblue]
	2051822474320 -> 2052233818272
	2052233818272 [label=AccumulateGrad]
	2052233818128 -> 2052233818080
	2051822474240 [label="conv3.4.weight
 (64)" fillcolor=lightblue]
	2051822474240 -> 2052233818128
	2052233818128 [label=AccumulateGrad]
	2052233817888 -> 2052233818080
	2051822480880 [label="conv3.4.bias
 (64)" fillcolor=lightblue]
	2051822480880 -> 2052233817888
	2052233817888 [label=AccumulateGrad]
	2052233817504 -> 2052233817600
	2052233817504 [label=TBackward0]
	2052233818032 -> 2052233817504
	2051822473920 [label="cnn_fc.1.weight
 (256, 16384)" fillcolor=lightblue]
	2051822473920 -> 2052233818032
	2052233818032 [label=AccumulateGrad]
	2052233817360 -> 2052233817216
	2052233817360 [label=ReluBackward0]
	2052233817936 -> 2052233817360
	2052233817936 [label=AddmmBackward0]
	2052233818704 -> 2052233817936
	2051822480240 [label="mlp.4.bias
 (32)" fillcolor=lightblue]
	2051822480240 -> 2052233818704
	2052233818704 [label=AccumulateGrad]
	2052233817792 -> 2052233817936
	2052233817792 [label=ReluBackward0]
	2052233818416 -> 2052233817792
	2052233818416 [label=NativeBatchNormBackward0]
	2052233818896 -> 2052233818416
	2052233818896 [label=AddmmBackward0]
	2052233819472 -> 2052233818896
	2051822480400 [label="mlp.0.bias
 (16)" fillcolor=lightblue]
	2051822480400 -> 2052233819472
	2052233819472 [label=AccumulateGrad]
	2052233819568 -> 2052233818896
	2052233819568 [label=TBackward0]
	2052233820336 -> 2052233819568
	2051822473760 [label="mlp.0.weight
 (16, 4)" fillcolor=lightblue]
	2051822473760 -> 2052233820336
	2052233820336 [label=AccumulateGrad]
	2052233819280 -> 2052233818416
	2051822480320 [label="mlp.1.weight
 (16)" fillcolor=lightblue]
	2051822480320 -> 2052233819280
	2052233819280 [label=AccumulateGrad]
	2052233819088 -> 2052233818416
	2051822473520 [label="mlp.1.bias
 (16)" fillcolor=lightblue]
	2051822473520 -> 2052233819088
	2052233819088 [label=AccumulateGrad]
	2052233817552 -> 2052233817936
	2052233817552 [label=TBackward0]
	2052233820144 -> 2052233817552
	2051822473600 [label="mlp.4.weight
 (32, 16)" fillcolor=lightblue]
	2051822473600 -> 2052233820144
	2052233820144 [label=AccumulateGrad]
	2052233817120 -> 2052233817312
	2052233817120 [label=TBackward0]
	2052233818224 -> 2052233817120
	2051822480160 [label="classifier.0.weight
 (64, 288)" fillcolor=lightblue]
	2051822480160 -> 2052233818224
	2052233818224 [label=AccumulateGrad]
	2052233817024 -> 2052233666880
	2052233817024 [label=TBackward0]
	2052233817744 -> 2052233817024
	2051822473280 [label="classifier.3.weight
 (5, 64)" fillcolor=lightblue]
	2051822473280 -> 2052233817744
	2052233817744 [label=AccumulateGrad]
	2052233666880 -> 2052233802880
}
